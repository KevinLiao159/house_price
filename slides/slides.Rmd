---
title: "Final Project - House Price"
author: 'Team Golden Squirrels: Lingjie Qiao, Minsu Kim, Kevin Liao and Cheng Peng'
date: "12/06/2016"
output:
  ioslides_presentation:
    fig_caption: yes
  beamer_presentation: default
---

## House Price -- Advanced Regression Techniques

* Statistics - National Association of Realtors
* Significance of The Project

<img src="../images/housesbanner.png" style="width: 750px; margin-top: 90px;"/>


## Agenda

* Project Objective
* Data Description
* Exploratory Data Analysis
* Methodology
* Feature Engineering
* Analysis and Key Findings
* Conclusions

## Project Objective
* Make full use of statistical models and predictive tools 
* Explore variable relationships, predict housing price and raise interesting questions from the dataset
    * **Prediction Accuracy**
    * **Model Interpretability**. 

## Data Description
Our analysis starts with the data sets provided by Kaggle Competition on  [House Price](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). There are three files:

* `data_description.txt` 
* `train.csv` 
* `test.csv` 

The dataset has 80 variables in total, including both categorical and numerical variables.

Our goal is to understand the relationship between `SalePrice` and these potential predictors with statistical modeling.


## Exploratory Data Analysis
With datasets in hand, the first thing we would like to explore is the dataset itself: what are some interesting relationships among variables and what human interpretation can we give to these observations? 

<img src="../images/eda-bath-vs-sale.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/historgram_original_price.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">


## Methodology

**Goal**: accurately predict the final price of each home

**Problem Framing**: regression problem

**Methods**: L2 loss function

* Data Preprocess 
* Feature Engineering
* Shrinkage models and ensemble models.

## Methodology

1. Objective Loss Function
<div class="blue2">
$$\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (log(p_{i} + 1) - log(a_{i} + 1))^2}$$
</div>

2. Data Preprocess
    - **Factorization**: dealing with categorical variables
    - **Missing Values**: dealing with NA values
    - **Log Transformation**: dealing with highly-skewed variables
    - **Outliers**: removing 17 outliers

## Data Preprocess - Log Transformation

<img src="../images/historgram_original_price.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/historgram_log_trasformed_price.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">

## Feature Engineering

**Problem 1**: Complexity of the Dataset

**Approach**: Dimension reduction and extraction of informative, feature engineered predictors

**Problem 2**: Small dataset

**Approach**: Manual feature engineering instead of feature learning

## Feature Engineering

<img src="../images/scatter_Year_sold_Year_remodeled_Vs_Price.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/scatter_Year_sold_Year_built_Vs_Price.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">


## Modeling

* **Shrinkage Regression Models** 
    * PCA - information compression
    * Lasso - L1 norm
    * Ridge - L2 norm

* **Ensemble Models**
    * Gradient Boosting Machine - Boosting 
    * Random Forest - Bagging with a random trick


## Principal Component Analysis

<img src="../images/model_pca_scree_plot.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/model_pca_pc_comparison_plot.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">

## MSE - Lasso and Ridge Regression

<img src="../images/model_lasso_lambda.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/model_ridge_lambda.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">

## Top 10 Coefficients for Lasso and Ridge

<img src="../images/model_lasso_coefficients_top_10.png" style="float: left; width: 47%; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/model_ridge_coefficients_top_10.png" style="float: right; width: 47%; margin-left: 1%; margin-bottom: 0.5em;">

## Model GBM Predictor Importance

<center>
<img src="../images/model_gbm_predictor_importance.png" style="width: 60%;">
</center>


## Results - Model Prediction Comparison

<img src="../images/model_prediction_comparison.png" style="float: left; width: 800px; margin-right: 1%; margin-bottom: 0.5em;">

<img src="../images/model_residual_comparison.png" style="float: left; width: 800px; margin-right: 1%; margin-bottom: 0.5em;">

## Results - RMSLE Comparison

```{r echo = FALSE, results='asis'}  
load("../data/cleanedData/RMSEL_Table.RData")
library(knitr)
xtable1 <- kable(model_comparison, caption = 'Table: RMSEL Comparison from Four Models')
print(xtable1, comment=FALSE, type = "latex", caption.placement = 'top')
```

\newline

**Random Forest** seems to have the smallest RMSLE

## Conclusions

In conclusions, it is feasible to predict the house price with more informative predictors we created. 

To emphasize collaborative teamwork and project reproducibility, we also wrote blog post and code documentation (kernel) so that more people can benefit and make use of our results.



## Acknowledgments

The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 

Special thanks to Professor Gaston Sanchez and GSI Sindhuja Jeyabal.

